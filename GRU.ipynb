{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding text\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "print(\"Start loading Word Vector Dictionary\")\n",
    "wordVec={}\n",
    "with open('glove.txt', encoding=\"utf8\") as glove:\n",
    "    count=0\n",
    "    for line in glove:\n",
    "        temp=line.split()\n",
    "        l=len(temp)\n",
    "        wordVec[' '.join(temp[0:l-50])]=list(map(np.float,temp[l-50:l]))\n",
    "        count=count+1\n",
    "        print(count)\n",
    "print(\"Finish loading Word Vector Dictionary\")\n",
    "print(\"Start loading training stances\\n\")\n",
    "csv.field_size_limit(999999999)\n",
    "with open('train_stances.csv', encoding=\"utf8\") as csvfile_stance:\n",
    "    stanceReader=csv.reader(csvfile_stance)\n",
    "    stances=[]\n",
    "    for row in stanceReader:\n",
    "        temp=[]\n",
    "        for c,c_ in zip(row[0],row[0][1:]):\n",
    "            if c.isalnum() or c.isspace():\n",
    "                temp.append(c.lower())\n",
    "            else:\n",
    "                if not c_.isspace():\n",
    "                    temp.append(\" \")\n",
    "        stances.append([''.join(temp),row[1],row[2]])\n",
    "print(\"Finish loading training stances\\n\")\n",
    "print(\"Start loading training bodies\\n\")\n",
    "with open('train_bodies.csv', encoding=\"utf8\") as csvfile_body:\n",
    "    bodyReader=csv.reader(csvfile_body)\n",
    "    bodies={}\n",
    "    for row in bodyReader:\n",
    "        temp=[]\n",
    "        for c,c_ in zip(row[1],row[1][1:]):\n",
    "            if c.isalnum() or c.isspace():\n",
    "                temp.append(c.lower())\n",
    "            else:\n",
    "                if not c_.isspace():\n",
    "                    temp.append(\" \")\n",
    "        bodies[row[0]]=''.join(temp)\n",
    "print(\"Finish loading training bodies\")\n",
    "print(\"Start merging training stances and training bodies\\n\")\n",
    "raw_training_set=[]\n",
    "for set in stances:\n",
    "    raw_training_set.append([set[0],bodies[set[1]],set[2]])\n",
    "    #print(bodies[key])\n",
    "\n",
    "with open(\"data.p\",\"wb\") as embedding:\n",
    "    pickle.dump(raw_training_set,embedding)\n",
    "print(\"Finish loading training stanes and training bodies\\n\")\n",
    "stanceReader=None\n",
    "bodyReader=None\n",
    "stances=None\n",
    "bodies=None\n",
    "print(\"Start embedding\\n\")\n",
    "\n",
    "with open('data_related.p', \"wb\") as embedded_data_related:\n",
    "    embedded_data_stances=open('data_stances.p','wb')\n",
    "    data_related=[]\n",
    "    data_stances=[] \n",
    "    for sample in raw_training_set:\n",
    "        title=sample[0].split()\n",
    "        body=sample[1].split()\n",
    "        label=sample[2]\n",
    "        titleVec=[]\n",
    "        bodyVec=[]\n",
    "        for i in range(len(title)):\n",
    "            #print(title[i] in wordVec)\n",
    "            if title[i] in wordVec:\n",
    "                word=wordVec[title[i]]\n",
    "                #print(wordVec[\"the\"])\n",
    "                titleVec.append(word)\n",
    "        for i in range(len(body)):\n",
    "            if body[i] in wordVec:\n",
    "                word=wordVec[body[i]]\n",
    "                bodyVec.append(word)\n",
    "        if label ==\"unrelated\":\n",
    "            data_related.append([titleVec,bodyVec,[1,0]])\n",
    "        else:\n",
    "            data_related.append([titleVec,bodyVec,[0,1]])\n",
    "            if label == \"discuss\":\n",
    "                label=[1,0,0]\n",
    "            if label == \"agree\":\n",
    "                label=[0,1,0]\n",
    "            if label == \"disagree\":\n",
    "                label=[0,0,1]\n",
    "            data_stances.append([titleVec,bodyVec,label])\n",
    "        #data.append([titleVec,bodyVec,label])\n",
    "    pickle.dump(np.array(data_related),embedded_data_related)\n",
    "    pickle.dump(np.array(data_stances),embedded_data_stances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#related-level1\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar 30 22:15:39 2018\n",
    "\n",
    "@author: HP\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "\n",
    "class dataWrapper:\n",
    "    def __init__(self,data):\n",
    "        self.size=len(data)\n",
    "        self.x_title=[]\n",
    "        self.x_body=[]\n",
    "        self.y=[]\n",
    "        self.seqlen_title=[]\n",
    "        self.seqlen_body=[]\n",
    "        self.current_batch=0\n",
    "        for sample in data:\n",
    "            self.x_title.append(sample[0])\n",
    "            self.x_body.append(sample[1])\n",
    "            self.y.append(sample[2])\n",
    "            self.seqlen_title.append(len(sample[0]))\n",
    "            self.seqlen_body.append(len(sample[1]))\n",
    "        max_seqlen=max(self.seqlen_body+self.seqlen_title)\n",
    "        #padding the samples with zero vectors\n",
    "        for i in range(len(self.x_title)):\n",
    "            self.x_title[i]+=[[0]*50]*(max_seqlen-len(self.x_title[i]))\n",
    "            self.x_body[i]+=[[0]*50]*(max_seqlen-len(self.x_body[i]))\n",
    "    # return the next batch of the data from the data set.\n",
    "    def next(self,batch_size):\n",
    "        if self.current_batch+batch_size<self.size:\n",
    "            self.current_batch+=batch_size\n",
    "            return self.x_title[self.current_batch:self.current_batch+batch_size],self.x_body[self.current_batch:self.current_batch+batch_size],self.y[self.current_batch:self.current_batch+batch_size],self.seqlen_title[self.current_batch:self.current_batch+batch_size],self.seqlen_body[self.current_batch:self.current_batch+batch_size]\n",
    "        else:\n",
    "            temp=self.current_batch\n",
    "            self.current_batch=self.current_batch+batch_size-self.size\n",
    "            batch_x_title=self.x_title[temp:]+self.x_title[:self.current_batch]\n",
    "            batch_x_body=self.x_body[temp:]+self.x_body[:self.current_batch]\n",
    "            batch_y=self.y[temp:]+self.y[:self.current_batch]\n",
    "            batch_seqlen_title=self.seqlen_title[temp:]+self.seqlen_title[:self.current_batch]\n",
    "            batch_seqlen_body=self.seqlen_body[temp:]+self.seqlen_body[:self.current_batch]\n",
    "            return batch_x_title,batch_x_body,batch_y,batch_seqlen_title,batch_seqlen_body\n",
    "    # return the length of the longest sequence\n",
    "    def max_seqlen(self):\n",
    "        return max(self.seqlen_body+self.seqlen_title)\n",
    "\n",
    "# loading the data\n",
    "data=pickle.load(open(\"data_stances.p\",\"rb\"))\n",
    "size=len(data)\n",
    "trainset=dataWrapper(data[size//3:])\n",
    "testset=dataWrapper(data[:size//3])\n",
    "\n",
    "#network parameters\n",
    "learning_rate=0.001\n",
    "training_iters=100000\n",
    "batch_size=128\n",
    "display_step=10\n",
    "\n",
    "seq_max_len=max(trainset.max_seqlen(),testset.max_seqlen())\n",
    "n_input=50\n",
    "n_hidden=60\n",
    "n_classes=3\n",
    "\n",
    "\n",
    "x_title=tf.placeholder(\"float\",[None,seq_max_len,n_input])\n",
    "x_body=tf.placeholder(\"float\",[None,seq_max_len,n_input])\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "seqlen_title=tf.placeholder(tf.int32,[None])\n",
    "seqlen_body=tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "weights={'out':tf.Variable(tf.random_normal([n_hidden,n_classes]))}\n",
    "biases={'out':tf.Variable(tf.random_normal([1,n_classes]))}\n",
    "\n",
    "def dynamicRNN(x_title,x_body,seqlen_title,seqlen_body,weights,biases):\n",
    "\n",
    "    gru_cell=rnn.GRUCell(num_units=n_hidden)\n",
    "    #rnn.BasicRNNCell()\n",
    "    print(\"testing_1\")\n",
    "    outputs_title,states_title=tf.nn.dynamic_rnn(cell=gru_cell,inputs=x_title,sequence_length=seqlen_title,dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('scope1',reuse=None):    \n",
    "        print(\"testing_2\")\n",
    "        outputs_body,states_body=tf.nn.dynamic_rnn(cell=gru_cell,inputs=x_body,sequence_length=seqlen_body,dtype=tf.float32)\n",
    "        print(\"testing_3\")\n",
    "        temp1=tf.stack([tf.range(tf.shape(seqlen_title)[0]),seqlen_title-1],axis=1)\n",
    "        temp2=tf.stack([tf.range(tf.shape(seqlen_body)[0]),seqlen_body-1],axis=1)\n",
    "        return tf.matmul(tf.multiply(tf.gather_nd(outputs_title,temp1),tf.gather_nd(outputs_body,temp2)),weights['out'])+biases['out']\n",
    "\n",
    "pred=dynamicRNN(x_title,x_body,seqlen_title,seqlen_body,weights,biases)\n",
    "cost =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "\n",
    "    while step*batch_size<training_iters:\n",
    "        print(\"step:\",step)\n",
    "\n",
    "        batch_x_title,batch_x_body,batch_y,batch_seqlen_title,batch_seqlen_body=trainset.next(batch_size)\n",
    "        sess.run(optimizer,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "        if step%display_step==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "            loss=sess.run(cost,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step+=1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "test_x_title=testset.x_title\n",
    "test_x_body=testset.x_body\n",
    "test_y=testset.y\n",
    "test_seqlen_title=testset.seqlen_title\n",
    "test_seqlen_body=testset.seqlen_body\n",
    "\n",
    "print(\"Test Accuracy:\",sess.run(accuracy,feed_dict={x_title:test_x_title,x_body:test_x_body,y:test_y,seqlen_title:test_seqlen_title,seqlen_body:test_seqlen_body}))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "import sys\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = sys.argv[1]\n",
    "class dataWrapper:\n",
    "    def __init__(self,data):\n",
    "        self.size=len(data)\n",
    "        self.x_title=[]\n",
    "        self.x_body=[]\n",
    "        self.y=[]\n",
    "        self.seqlen_title=[]\n",
    "        self.seqlen_body=[]\n",
    "        self.current_batch=0\n",
    "        for sample in data:\n",
    "            self.x_title.append(sample[0])\n",
    "            self.x_body.append(sample[1])\n",
    "            self.y.append(sample[2])\n",
    "            self.seqlen_title.append(len(sample[0]))\n",
    "            self.seqlen_body.append(len(sample[1]))\n",
    "        max_seqlen=6000\n",
    "        #padding the samples with zero vectors\n",
    "        for i in range(len(self.x_title)):\n",
    "            self.x_title[i]+=[[0]*50]*(max_seqlen-len(self.x_title[i]))\n",
    "            self.x_body[i]+=[[0]*50]*(max_seqlen-len(self.x_body[i]))\n",
    "    # return the next batch of the data from the data set.\n",
    "    def next(self,batch_size):\n",
    "        if self.current_batch+batch_size<self.size:\n",
    "            self.current_batch+=batch_size\n",
    "            return self.x_title[self.current_batch:self.current_batch+batch_size],self.x_body[self.current_batch:self.current_batch+batch_size],self.y[self.current_batch:self.current_batch+batch_size],self.seqlen_title[self.current_batch:self.current_batch+batch_size],self.seqlen_body[self.current_batch:self.current_batch+batch_size]\n",
    "        else:\n",
    "            temp=self.current_batch\n",
    "            self.current_batch=self.current_batch+batch_size-self.size\n",
    "            batch_x_title=self.x_title[temp:]+self.x_title[:self.current_batch]\n",
    "            batch_x_body=self.x_body[temp:]+self.x_body[:self.current_batch]\n",
    "            batch_y=self.y[temp:]+self.y[:self.current_batch]\n",
    "            batch_seqlen_title=self.seqlen_title[temp:]+self.seqlen_title[:self.current_batch]\n",
    "            batch_seqlen_body=self.seqlen_body[temp:]+self.seqlen_body[:self.current_batch]\n",
    "            return batch_x_title,batch_x_body,batch_y,batch_seqlen_title,batch_seqlen_body\n",
    "    # return the length of the longest sequence\n",
    "    def max_seqlen(self):\n",
    "        return max(self.seqlen_body+self.seqlen_title)\n",
    "\n",
    "# loading the data\n",
    "data=pickle.load(open(\"data_stances.p\",\"rb\"))\n",
    "size=len(data)\n",
    "trainset=np.array(data[800:])\n",
    "np.random.shuffle(trainset)\n",
    "trainset=dataWrapper(trainset)\n",
    "testset=dataWrapper(data[:800])\n",
    "data=None\n",
    "#network parameters\n",
    "learning_rate=0.001\n",
    "#training_iters=100000\n",
    "training_iters=3*trainset.size\n",
    "batch_size=128\n",
    "display_step=10\n",
    "\n",
    "seq_max_len=6000\n",
    "n_input=50\n",
    "n_hidden=100\n",
    "n_classes=3\n",
    "\n",
    "\n",
    "x_title=tf.placeholder(\"float\",[None,seq_max_len,n_input])\n",
    "x_body=tf.placeholder(\"float\",[None,seq_max_len,n_input])\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "seqlen_title=tf.placeholder(tf.int32,[None])\n",
    "seqlen_body=tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "weights={'out':tf.Variable(tf.random_normal([n_hidden,n_classes]))}\n",
    "biases={'out':tf.Variable(tf.random_normal([1,n_classes]))}\n",
    "\n",
    "def dynamicRNN(x_title,x_body,seqlen_title,seqlen_body,weights,biases):\n",
    "\n",
    "    lstm_cell=rnn.GRUCell(n_hidden)\n",
    "    print(\"testing_1\")\n",
    "    outputs_title,states_title=tf.nn.dynamic_rnn(cell=lstm_cell,inputs=x_title,sequence_length=seqlen_title,dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('scope2',reuse=None):    \n",
    "        print(\"testing_2\")\n",
    "        outputs_body,states_body=tf.nn.dynamic_rnn(cell=lstm_cell,inputs=x_body,sequence_length=seqlen_body,dtype=tf.float32)\n",
    "        print(\"testing_3\")\n",
    "        temp1=tf.stack([tf.range(tf.shape(seqlen_title)[0]),seqlen_title-1],axis=1)\n",
    "        temp2=tf.stack([tf.range(tf.shape(seqlen_body)[0]),seqlen_body-1],axis=1)\n",
    "        return tf.matmul(tf.multiply(tf.gather_nd(outputs_title,temp1),tf.gather_nd(outputs_body,temp2)),weights['out'])+biases['out']\n",
    "\n",
    "pred=dynamicRNN(x_title,x_body,seqlen_title,seqlen_body,weights,biases)\n",
    "cost =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.device('/device:GPU:0'):\n",
    "    sess=tf.Session(config=config)\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "    saver.restore(sess, \"/uac/y15/kcli5/FNC-1/tmp2/model_stance_1.0-290\")\n",
    "    print(\"Model restored.\")\n",
    "    while step*batch_size<training_iters:\n",
    "        print(\"step:\",step)\n",
    "\n",
    "        batch_x_title,batch_x_body,batch_y,batch_seqlen_title,batch_seqlen_body=trainset.next(batch_size)\n",
    "        sess.run(optimizer,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "        if step%display_step==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "            loss=sess.run(cost,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "            save_path = saver.save(sess, \"/uac/y15/kcli5/FNC-1/tmp2/model_stance_1.0\",global_step=step+540)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "        step+=1\n",
    "    print(\"Optimization Finished!\")\n",
    "    test_x_title=testset.x_title\n",
    "    test_x_body=testset.x_body\n",
    "    test_y=testset.y\n",
    "    test_seqlen_title=testset.seqlen_title\n",
    "    test_seqlen_body=testset.seqlen_body\n",
    "    print(\"Test Accuracy:\",sess.run(accuracy,feed_dict={x_title:test_x_title,x_body:test_x_body,y:test_y,seqlen_title:test_seqlen_title,seqlen_body:test_seqlen_body}))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "\n",
    "class dataWrapper:\n",
    "    def __init__(self,data):\n",
    "        self.size=len(data)\n",
    "        self.x_title=[]\n",
    "        self.x_body=[]\n",
    "        self.y=[]\n",
    "        self.seqlen_title=[]\n",
    "        self.seqlen_body=[]\n",
    "        self.current_batch=0\n",
    "        for sample in data:\n",
    "            self.x_title.append(sample[0])\n",
    "            self.x_body.append(sample[1])\n",
    "            self.y.append(sample[2])\n",
    "            self.seqlen_title.append(len(sample[0]))\n",
    "            self.seqlen_body.append(len(sample[1]))\n",
    "        max_seqlen=max(self.seqlen_body+self.seqlen_title)\n",
    "        #padding the samples with zero vectors\n",
    "        for i in range(len(self.x_title)):\n",
    "            self.x_title[i]+=[[0]*50]*(max_seqlen-len(self.x_title[i]))\n",
    "            self.x_body[i]+=[[0]*50]*(max_seqlen-len(self.x_body[i]))\n",
    "    # return the next batch of the data from the data set.\n",
    "    def next(self,batch_size):\n",
    "        if self.current_batch+batch_size<self.size:\n",
    "            self.current_batch+=batch_size\n",
    "            return self.x_title[self.current_batch:self.current_batch+batch_size],self.x_body[self.current_batch:self.current_batch+batch_size],self.y[self.current_batch:self.current_batch+batch_size],self.seqlen_title[self.current_batch:self.current_batch+batch_size],self.seqlen_body[self.current_batch:self.current_batch+batch_size]\n",
    "        else:\n",
    "            temp=self.current_batch\n",
    "            self.current_batch=self.current_batch+batch_size-self.size\n",
    "            batch_x_title=self.x_title[temp:]+self.x_title[:self.current_batch]\n",
    "            batch_x_body=self.x_body[temp:]+self.x_body[:self.current_batch]\n",
    "            batch_y=self.y[temp:]+self.y[:self.current_batch]\n",
    "            batch_seqlen_title=self.seqlen_title[temp:]+self.seqlen_title[:self.current_batch]\n",
    "            batch_seqlen_body=self.seqlen_body[temp:]+self.seqlen_body[:self.current_batch]\n",
    "            return batch_x_title,batch_x_body,batch_y,batch_seqlen_title,batch_seqlen_body\n",
    "    # return the length of the longest sequence\n",
    "    def max_seqlen(self):\n",
    "        return max(self.seqlen_body+self.seqlen_title)\n",
    "\n",
    "# loading the data\n",
    "data=pickle.load(open(\"data_stances.p\",\"rb\"))\n",
    "size=len(data)\n",
    "trainset=dataWrapper(data[size//3:])\n",
    "testset=dataWrapper(data[:size//3])\n",
    "\n",
    "#network parameters\n",
    "learning_rate=0.001\n",
    "training_iters=100000\n",
    "batch_size=128\n",
    "display_step=10\n",
    "\n",
    "seq_max_len=max(trainset.max_seqlen(),testset.max_seqlen())\n",
    "n_input=50\n",
    "n_hidden=60\n",
    "n_classes=3\n",
    "\n",
    "\n",
    "x_title=tf.placeholder(\"float\",[None,seq_max_len,n_input])\n",
    "x_body=tf.placeholder(\"float\",[None,seq_max_len,n_input])\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "seqlen_title=tf.placeholder(tf.int32,[None])\n",
    "seqlen_body=tf.placeholder(tf.int32,[None])\n",
    "\n",
    "\n",
    "weights={'out':tf.Variable(tf.random_normal([n_hidden,n_classes]))}\n",
    "biases={'out':tf.Variable(tf.random_normal([1,n_classes]))}\n",
    "\n",
    "def dynamicRNN(x_title,x_body,seqlen_title,seqlen_body,weights,biases):\n",
    "\n",
    "    gru_cell=rnn.GRUCell(n_hidden)\n",
    "    print(\"testing_1\")\n",
    "    outputs_title,states_title=tf.nn.dynamic_rnn(cell=gru_cell,inputs=x_title,sequence_length=seqlen_title,dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('scope1',reuse=None):    \n",
    "        print(\"testing_2\")\n",
    "        outputs_body,states_body=tf.nn.dynamic_rnn(cell=gru_cell,inputs=x_body,sequence_length=seqlen_body,dtype=tf.float32)\n",
    "        print(\"testing_3\")\n",
    "        temp1=tf.stack([tf.range(tf.shape(seqlen_title)[0]),seqlen_title-1],axis=1)\n",
    "        temp2=tf.stack([tf.range(tf.shape(seqlen_body)[0]),seqlen_body-1],axis=1)\n",
    "        return tf.matmul(tf.multiply(tf.gather_nd(outputs_title,temp1),tf.gather_nd(outputs_body,temp2)),weights['out'])+biases['out']\n",
    "\n",
    "pred=dynamicRNN(x_title,x_body,seqlen_title,seqlen_body,weights,biases)\n",
    "cost =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "\n",
    "    while step*batch_size<training_iters:\n",
    "        print(\"step:\",step)\n",
    "\n",
    "        batch_x_title,batch_x_body,batch_y,batch_seqlen_title,batch_seqlen_body=trainset.next(batch_size)\n",
    "        sess.run(optimizer,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "        if step%display_step==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "            loss=sess.run(cost,feed_dict={x_title:batch_x_title,x_body:batch_x_body,y:batch_y,seqlen_title:batch_seqlen_title,seqlen_body:batch_seqlen_body})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step+=1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "test_x_title=testset.x_title\n",
    "test_x_body=testset.x_body\n",
    "test_y=testset.y\n",
    "test_seqlen_title=testset.seqlen_title\n",
    "test_seqlen_body=testset.seqlen_body\n",
    "\n",
    "print(\"Test Accuracy:\",sess.run(accuracy,feed_dict={x_title:test_x_title,x_body:test_x_body,y:test_y,seqlen_title:test_seqlen_title,seqlen_body:test_seqlen_body}))\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
