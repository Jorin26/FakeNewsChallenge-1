{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A small meteorite crashed into a wooded area in Nicaraguas capital of Managua overnight the government said Sunday Residents reported hearing a mysterious boom that left a 16foot deep crater near the citys airport the Associated Press reports Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a relatively small meteorite that appears to have come off an asteroid that was passing close to Earth Housesized asteroid 2014 RC which measured 60 feet in diameter skimmed the Earth this weekend ABC News reports Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet said Humberto Saballos a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee He said it is still not clear if the meteorite disintegrated or was buried Humberto Garcia of the Astronomy Center at the National Autonomous University of Nicaragua said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night We have to study it more because it could be ice or rock he said Wilfried Strauch an adviser to the Institute of Territorial Studies said it was very strange that no one reported a streak of light We have to ask if anyone has a photo or something Local residents reported hearing a loud boom Saturday night but said they didnt see anything strange in the sky I was sitting on my porch and I saw nothing then all of a sudden I heard a large blast We thought it was a bomb because we felt an expansive wave Jorge Santamaria told The Associated Press The site of the crater is near Managuas international airport and an air force base Only journalists from state media were allowed to visit it\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "with open('train_bodies.csv',encoding='latin1') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    articles={}\n",
    "    for i in spamreader:\n",
    "        articles[i[0]]=i[1]\n",
    "del articles['Body ID']  \n",
    "def clean_data(body):\n",
    "    punctuations = string.punctuation + '—' + '’' + '…' + '‘' + '–' + '”' + '“'\n",
    "    regex = re.compile('[%s]' % re.escape(punctuations))\n",
    "    clean_text=''\n",
    "    for sentence in sent_tokenize(body):\n",
    "        sentence=regex.sub('', sentence)\n",
    "        sentence=re.sub(r\"\\n\", \" \", sentence)\n",
    "        sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "        sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "        sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "        sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "        sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "        sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "        sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "        sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "        sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "        sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "        sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "        sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "        sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "        sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "        sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "        sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "        sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "        sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "        sentence = re.sub(r\"'til\", \"until\", sentence)\n",
    "        sentence = re.sub(' +',' ',sentence)\n",
    "        clean_text=clean_text+' '+sentence\n",
    "    return clean_text\n",
    "    #print(clean_text)\n",
    "\n",
    " \n",
    "clean_articles={}\n",
    "for i in articles:\n",
    "    clean_articles[int(i)]=clean_data(articles[i])\n",
    "\n",
    "print(clean_articles[0])\n",
    "clean_articles_list=[]\n",
    "single_corpus=''\n",
    "for i in clean_articles:\n",
    "    clean_articles_list.append(clean_articles[i])\n",
    "    single_corpus=single_corpus+' '+clean_articles[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confirmationâ\\x80\\x9d', 'weirdly', 'vigorous', 'antiMuslim', 'orchestrating', 'requesting', 'surpassing', 'apiece', 'Shebab', 'Keith182', 'Obviously', '20413', 'tanir', '4m', 'dont', 'Guerrero', 'archrival', 'spitting', 'momentum']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vocab=set(single_corpus.split())\n",
    "vocab = [w for w in vocab if not w in stop_words]\n",
    "print(vocab[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Last week we hinted at what was to come as Ebola fears spread across America Today we get confirmation As The Daily Caller reports one passenger at Dulles International Airport outside Washington DC is apparently not taking any chances A female passenger dressed in a hazmat suit complete with a full body gown mask and gloves was spotted Wednesday waiting for a flight at the airport Source The Daily Caller We particularly liked the JCPenney bag maybe thats a new business line for the bankrupt retailer On a side note try Halloween stores if you need a HazMat suit in a hurry\n"
     ]
    }
   ],
   "source": [
    "#get stances\n",
    "with open('train_stances.csv',encoding='latin1') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    stances={}\n",
    "    title={}\n",
    "    for i in spamreader:\n",
    "        #print(i[1])\n",
    "        stances[i[1]]=i[2]\n",
    "        title[i[1]]=i[0]\n",
    "#print(stances['0'])\n",
    "del stances['Body ID']\n",
    "del title['Body ID']\n",
    "stances = {int(k):v for k,v in stances.items()}\n",
    "title = {int(k):v for k,v in title.items()}\n",
    "\n",
    "final_data=[]\n",
    "for i in clean_articles:\n",
    "    final_data.append([i,clean_articles[i],stances[i]])\n",
    "#print(final_data[1])\n",
    "#print(stances)\n",
    "#print(clean_articles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#corpus = [   'This is the first document.',     'This is the second second document.',     'And the third one.',     'Is this the first document?',]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "t=vectorizer.fit_transform(clean_articles_list)\n",
    "vect_articles=t.toarray()\n",
    "print(vect_articles)\n",
    "y_ids=[]\n",
    "for i in clean_articles:\n",
    "    y_ids.append(i)\n",
    "stance_rearranged=[]\n",
    "for i in y_ids:\n",
    "    stance_rearranged.append(stances[i])\n",
    "#print(stance_rearranged)\n",
    "y_stance_label=[]\n",
    "for s in stance_rearranged:\n",
    "    if s=='unrelated':\n",
    "        y_stance_label.append(1)\n",
    "    else :\n",
    "        y_stance_label.append(0)\n",
    "        \n",
    "print(y_stance_label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.16493188] [[ 0.          0.         -1.58328394 ...  0.          5.43633955\n",
      "   0.        ]]\n",
      "Accuracy from sk-learn: 0.9952465834818776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, C = 1e15)\n",
    "clf.fit(vect_articles, y_stance_label)\n",
    "\n",
    "print(clf.intercept_, clf.coef_)\n",
    "#print(weights)\n",
    "print('Accuracy from sk-learn: {0}'.format(clf.score(vect_articles, y_stance_label)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0\n",
      " 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1\n",
      " 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1\n",
      " 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1\n",
      " 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1\n",
      " 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0\n",
      " 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
      " 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
      " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
      " 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
      " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0\n",
      " 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0\n",
      " 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
      " 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
      " 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "with open('competition_test_bodies.csv',encoding='latin1') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    articles_test={}\n",
    "    for i in spamreader:\n",
    "        articles_test[i[0]]=i[1]\n",
    "del articles_test['Body ID']  \n",
    "def clean_data(body):\n",
    "    punctuations = string.punctuation + '—' + '’' + '…' + '‘' + '–' + '”' + '“'\n",
    "    regex = re.compile('[%s]' % re.escape(punctuations))\n",
    "    clean_text=''\n",
    "    for sentence in sent_tokenize(body):\n",
    "        sentence=regex.sub('', sentence)\n",
    "        sentence=re.sub(r\"\\n\", \" \", sentence)\n",
    "        sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "        sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "        sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "        sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "        sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "        sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "        sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "        sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "        sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "        sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "        sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "        sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "        sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "        sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "        sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "        sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "        sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "        sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "        sentence = re.sub(r\"'til\", \"until\", sentence)\n",
    "        sentence = re.sub(' +',' ',sentence)\n",
    "        clean_text=clean_text+' '+sentence\n",
    "    return clean_text\n",
    "    #print(clean_text)\n",
    "\n",
    " \n",
    "clean_articles_test={}\n",
    "for i in articles_test:\n",
    "    clean_articles_test[int(i)]=clean_data(articles_test[i])\n",
    "\n",
    "#print(clean_articles[0])\n",
    "clean_articles_list_test=[]\n",
    "for i in clean_articles_test:\n",
    "    clean_articles_list_test.append(clean_articles_test[i])\n",
    "\n",
    "t=vectorizer.fit_transform(clean_articles_list_test)\n",
    "vect_articles_test=t.toarray()\n",
    "print(vect_articles_test)\n",
    "\n",
    "\n",
    "print(clf.predict(X=vect_articles_test))\n",
    "#print('Accuracy from sk-learn: {0}'.format(clf.score(vect_articles_test, y_stance_label)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
