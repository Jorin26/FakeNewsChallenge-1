{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for cleaning a article\n",
    "\n",
    "def clean_data(body):\n",
    "    punctuations = string.punctuation + '—' + '’' + '…' + '‘' + '–' + '”' + '“'\n",
    "    regex = re.compile('[%s]' % re.escape(punctuations))\n",
    "    clean_text=''\n",
    "    for sentence in sent_tokenize(body):\n",
    "        sentence=regex.sub('', sentence)\n",
    "        sentence=re.sub(r\"\\n\", \" \", sentence)\n",
    "        sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "        sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "        sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "        sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "        sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "        sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "        sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "        sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "        sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "        sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "        sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "        sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "        sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "        sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "        sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "        sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "        sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "        sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "        sentence = re.sub(r\"'til\", \"until\", sentence)\n",
    "        sentence = re.sub(' +',' ',sentence)\n",
    "        clean_text=clean_text+' '+sentence\n",
    "    return clean_text\n",
    "    \n",
    "#clean all articles\n",
    "def clean_bodies(articles):\n",
    "    clean_articles={}\n",
    "    for i in articles:\n",
    "        clean_articles[int(i)]=clean_data(articles[i])\n",
    "    #print(clean_articles[0])\n",
    "    clean_articles_list=[]\n",
    "    for i in clean_articles:\n",
    "        clean_articles_list.append(clean_articles[i])\n",
    "    return(clean_articles_list)\n",
    "\n",
    "\n",
    "#read the articles\n",
    "\n",
    "def read_bodies(file_name):\n",
    "    with open(file_name,encoding='latin1') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        articles={}\n",
    "        for i in spamreader:\n",
    "            articles[i[0]]=i[1]\n",
    "    del articles['Body ID']\n",
    "    clean_articles={}\n",
    "    for i in articles:\n",
    "        clean_articles[int(i)]=clean_data(articles[i])\n",
    "    #print(clean_articles[0])\n",
    "    clean_articles_list=[]\n",
    "    for i in clean_articles:\n",
    "        clean_articles_list.append(clean_articles[i])\n",
    "    return clean_articles,clean_articles_list\n",
    "\n",
    "#create vocabulary\n",
    "\n",
    "def generate_vocab(list_articles):\n",
    "    single_string=''\n",
    "    for article in list_articles:\n",
    "        single_string=single_string + ' ' + article\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    vocab=set(single_string.split())\n",
    "    \n",
    "    vocab = [w for w in vocab if not w in stop_words]\n",
    "   \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    new_vocab=[]\n",
    "    for w in vocab:\n",
    "        new_vocab.append(wordnet_lemmatizer.lemmatize(w))\n",
    "    new_vocab=set(new_vocab)\n",
    "    return vocab\n",
    "\n",
    "#read title and stances into dictionaries\n",
    "def read_title_stances(filename):\n",
    "    with open(filename,encoding='latin1') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        stances={}\n",
    "        title={}\n",
    "        for i in spamreader:\n",
    "            stances[i[1]]=i[2]\n",
    "            title[i[1]]=i[0]\n",
    "        del title['Body ID']\n",
    "        del stances['Body ID']\n",
    "        title = {int(k):v for k,v in title.items()}\n",
    "        stances = {int(k):v for k,v in stances.items()}\n",
    "    return title,stances\n",
    "\n",
    "#create tf-idf matrix of features\n",
    "def generate_matrix(article_list,vocabulary):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "    t=vectorizer.fit_transform(article_list)\n",
    "    vect_articles=t.toarray()\n",
    "    #print(vect_articles)\n",
    "    return vect_articles\n",
    "\n",
    "#convert binary lables- 1 for selected stance 0 for rest\n",
    "def convert_labels(article,stances,selected_stance):\n",
    "    y_ids=[]\n",
    "    for i in article:\n",
    "        y_ids.append(i)\n",
    "    stance_rearranged=[]\n",
    "    for i in y_ids:\n",
    "        stance_rearranged.append(stances[i])\n",
    "    y_stance_label=[]\n",
    "    for s in stance_rearranged:\n",
    "        if s==selected_stance:\n",
    "            y_stance_label.append(1)\n",
    "        else :\n",
    "            y_stance_label.append(0)\n",
    "    return y_stance_label\n",
    "\n",
    "#calculate accuracy of prediction\n",
    "def calculate_accuracy(predictions,labels):\n",
    "    count=0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]==labels[i]:\n",
    "            count+=1\n",
    "    #print(sum(predictions),'\\n',sum(labels))\n",
    "    return count/len(labels)\n",
    "\n",
    "#reorder stances by article bodies\n",
    "def order_stance_by_body(article,stances):\n",
    "    y_ids=[]\n",
    "    for i in article:\n",
    "        y_ids.append(i)\n",
    "    stance_rearranged=[]\n",
    "    for i in y_ids:\n",
    "        stance_rearranged.append(stances[i])\n",
    "    return stance_rearranged\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_regression(training_features, label, num_steps, learning_rate,c, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((training_features.shape[0], 1))\n",
    "        training_features = np.hstack((intercept, training_features))\n",
    "    \n",
    "    weights = np.zeros(training_features.shape[1])\n",
    "    #c=100\n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(training_features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with gradient\n",
    "        output_error = label - predictions\n",
    "        gradient = np.dot(training_features.T, output_error)+ (2*c*weights)\n",
    "        weights += learning_rate * gradient\n",
    "        \n",
    "    return weights[0],weights[1:]\n",
    "\n",
    "#calculate final scores for prediction\n",
    "def calculate_results(x,weight,intercept):\n",
    "    #data_with_intercept = np.hstack((np.ones((x.shape[0], 1)), x))                                \n",
    "    final_scores = np.dot(x, np.transpose(weight))+intercept\n",
    "    predictions=[]\n",
    "    for score in final_scores: \n",
    "        predictions.append(sigmoid(score))\n",
    "    #final_scores.sigmoid()\n",
    "    return predictions\n",
    "\n",
    "#calculate weights and intercept\n",
    "def calulate_weights(selected_stance,articles_train,stances,vect_articles):\n",
    "\n",
    "    y_stance_label=convert_labels(article=articles_train,selected_stance = selected_stance, stances=stances)\n",
    "\n",
    "    intercept , weights = logistic_regression(training_features=vect_articles,label=y_stance_label,\n",
    "                         num_steps = 4000, learning_rate = 5e-6, add_intercept=True , c=100)\n",
    "    \n",
    "    return intercept , weights\n",
    "\n",
    "#order the titles by article bodies\n",
    "def order_title_by_body(article,title):\n",
    "    y_ids=[]\n",
    "    for i in article:\n",
    "        y_ids.append(i)\n",
    "    title_rearranged=[]\n",
    "    for i in y_ids:\n",
    "        title_rearranged.append(title[i])\n",
    "    return title_rearranged\n",
    "\n",
    "#append the title to the article body\n",
    "def append_title(article_list,title):\n",
    "    for i in range(len(article_list)):\n",
    "        article_list[i] = article_list[i]+' '+ title[i]\n",
    "    return article_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_train , article_list_train = read_bodies('train_bodies.csv')\n",
    "\n",
    "title_train , stances_train = read_title_stances('train_stances.csv')\n",
    "\n",
    "vocab_train = generate_vocab(article_list_train)\n",
    "\n",
    "title_train_rearranged = order_title_by_body(article = articles_train , title = title_train)\n",
    "\n",
    "article_list_train = append_title(article_list = article_list_train , title = title_train_rearranged )\n",
    "\n",
    "train_matrix = generate_matrix(article_list=article_list_train,vocabulary=vocab_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_test , article_list_test = read_bodies('competition_test_bodies.csv')\n",
    "\n",
    "title_test , stances_test =read_title_stances(filename='competition_test_stances.csv')\n",
    "\n",
    "title_test_rearranged = order_title_by_body(article = articles_test , title = title_test )\n",
    "\n",
    "article_list_test = append_title(article_list = article_list_test , title = title_test_rearranged )\n",
    "\n",
    "test_matrix=generate_matrix(article_list = article_list_test , vocabulary=vocab_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13752231 -5.31850756 -0.59484479] [[ 0.          0.74983346  0.         ...  0.         -0.46739639\n",
      "  -0.79210826]\n",
      " [ 0.         -0.00474117  0.         ...  0.         -0.02063156\n",
      "   0.14040751]\n",
      " [ 0.         -0.7749598   0.         ...  0.          0.51205094\n",
      "   0.61615012]]\n",
      "Accuracy from sk-learn: 0.6603982300884956\n"
     ]
    }
   ],
   "source": [
    "#result using sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_s=order_stance_by_body(article= articles_test , stances = stances_test)\n",
    "\n",
    "y_s=order_stance_by_body(article=articles_train,stances=stances_train)\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, C = 100,multi_class = 'ovr')\n",
    "\n",
    "clf.fit(train_matrix, y_s)\n",
    "\n",
    "pred = clf.predict(X=test_matrix)\n",
    "\n",
    "print(clf.intercept_, clf.coef_)\n",
    "\n",
    "print('Accuracy from sk-learn: {0}'.format(calculate_accuracy(labels = test_s , predictions = pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "intercept_agree , w_agree = calulate_weights(selected_stance = 'agree' , articles_train = articles_train , \n",
    "                                         stances = stances_train , vect_articles = train_matrix )\n",
    "\n",
    "intercept_disagree , w_disagree = calulate_weights(selected_stance = 'disagree', articles_train=articles_train , \n",
    "                                                   stances = stances_train , vect_articles = train_matrix )\n",
    "\n",
    "intercept_discuss , w_discuss = calulate_weights(selected_stance = 'discuss', articles_train = articles_train ,\n",
    "                                                  stances = stances_train , vect_articles = train_matrix )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "score_agree = calculate_results( weight=w_agree , x=test_matrix , intercept = intercept_agree )\n",
    "\n",
    "score_disagree = calculate_results( weight=w_disagree , x=test_matrix , intercept=intercept_disagree )\n",
    "\n",
    "score_discuss=calculate_results(weight = w_discuss , x = test_matrix , intercept = intercept_discuss )\n",
    "#print((score_unrelated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=[]\n",
    "\n",
    "for i in range(len(score_agree)):\n",
    "\n",
    "    m=max(score_agree[i],score_disagree[i],score_discuss[i])\n",
    "    \n",
    "    if m==score_agree[i]:\n",
    "        result.append('agree')\n",
    "    \n",
    "    elif m==score_discuss[i]:\n",
    "        result.append('discuss')\n",
    "    \n",
    "    else:\n",
    "        result.append('disagree')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy with 1 vs all Logistic Regression =  0.5995575221238938\n"
     ]
    }
   ],
   "source": [
    "#result using logistic regression function\n",
    "actual_stance = order_stance_by_body( article = articles_test , stances = stances_test)\n",
    "\n",
    "acc=calculate_accuracy( labels = actual_stance , predictions = result ) \n",
    "\n",
    "print('Final Accuracy with 1 vs all Logistic Regression = ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33795\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1e-15\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1.0000000000000002e-14\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1e-13\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1e-12\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1.0000000000000001e-11\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1e-10\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1e-09\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1e-08\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1.0000000000000001e-07\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1.0000000000000002e-06\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1e-05\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  0.0001\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  0.001\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  0.01\n",
      "0.5923945335710041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.00      0.00      0.00       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.59      1.00      0.74       997\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  0.1\n",
      "0.6054664289958408\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.92      0.04      0.07       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.60      1.00      0.75       997\n",
      "\n",
      "avg / total       0.68      0.61      0.47      1683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  1.0\n",
      "0.750445632798574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.71      0.63      0.67       590\n",
      "   disagree       0.00      0.00      0.00        96\n",
      "    discuss       0.77      0.89      0.83       997\n",
      "\n",
      "avg / total       0.70      0.75      0.72      1683\n",
      "\n",
      "For C:  10.0\n",
      "0.7605466428995841\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.71      0.68      0.69       590\n",
      "   disagree       0.50      0.12      0.20        96\n",
      "    discuss       0.79      0.87      0.83       997\n",
      "\n",
      "avg / total       0.75      0.76      0.75      1683\n",
      "\n",
      "For C:  100.00000000000001\n",
      "0.750445632798574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.69      0.66      0.67       590\n",
      "   disagree       0.52      0.18      0.26        96\n",
      "    discuss       0.79      0.86      0.82       997\n",
      "\n",
      "avg / total       0.74      0.75      0.74      1683\n",
      "\n",
      "For C:  1000.0000000000001\n",
      "0.7450980392156863\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.68      0.66      0.67       590\n",
      "   disagree       0.46      0.19      0.27        96\n",
      "    discuss       0.79      0.85      0.82       997\n",
      "\n",
      "avg / total       0.73      0.75      0.73      1683\n",
      "\n",
      "For C:  10000.0\n",
      "0.7415329768270945\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.68      0.65      0.66       590\n",
      "   disagree       0.41      0.18      0.25        96\n",
      "    discuss       0.79      0.85      0.82       997\n",
      "\n",
      "avg / total       0.73      0.74      0.73      1683\n",
      "\n",
      "For C:  100000.00000000001\n",
      "0.7391562685680333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.67      0.65      0.66       590\n",
      "   disagree       0.39      0.18      0.24        96\n",
      "    discuss       0.79      0.84      0.82       997\n",
      "\n",
      "avg / total       0.73      0.74      0.73      1683\n",
      "\n",
      "For C:  1000000.0000000001\n",
      "0.7361853832442068\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.67      0.65      0.66       590\n",
      "   disagree       0.39      0.18      0.24        96\n",
      "    discuss       0.79      0.84      0.81       997\n",
      "\n",
      "avg / total       0.72      0.74      0.73      1683\n",
      "\n",
      "For C:  10000000.0\n",
      "0.7361853832442068\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.67      0.65      0.66       590\n",
      "   disagree       0.39      0.18      0.24        96\n",
      "    discuss       0.79      0.84      0.81       997\n",
      "\n",
      "avg / total       0.72      0.74      0.73      1683\n",
      "\n",
      "For C:  100000000.0\n",
      "0.7361853832442068\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.67      0.65      0.66       590\n",
      "   disagree       0.39      0.18      0.24        96\n",
      "    discuss       0.79      0.84      0.81       997\n",
      "\n",
      "avg / total       0.72      0.74      0.73      1683\n",
      "\n",
      "For C:  1000000000.0000001\n",
      "0.7361853832442068\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.67      0.65      0.66       590\n",
      "   disagree       0.39      0.18      0.24        96\n",
      "    discuss       0.79      0.84      0.81       997\n",
      "\n",
      "avg / total       0.72      0.74      0.73      1683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, cross_validation\n",
    "mult = 1\n",
    "C = 1e-15\n",
    "list_acc={}\n",
    "while C * mult <= 1e10:\n",
    "    clf = LogisticRegression(fit_intercept=True, C = C * mult, multi_class = 'ovr')\n",
    "    predicted = cross_validation.cross_val_predict(clf, train_matrix, y_s, cv=10)\n",
    "    list_acc[C*mult]= metrics.accuracy_score(y_s, predicted)\n",
    "    print(\"For C: \", C * mult)\n",
    "    print(metrics.accuracy_score(y_s, predicted))\n",
    "    print(metrics.classification_report(y_s, predicted)) \n",
    "    mult = mult * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-10 \t 0.5923945335710041\n",
      "1e-09 \t 0.5923945335710041\n",
      "1e-08 \t 0.5923945335710041\n",
      "1.0000000000000001e-07 \t 0.5923945335710041\n",
      "1e-06 \t 0.5923945335710041\n",
      "1e-05 \t 0.5923945335710041\n",
      "0.0001 \t 0.5923945335710041\n",
      "0.001 \t 0.5923945335710041\n",
      "0.01 \t 0.5923945335710041\n",
      "0.1 \t 0.6054664289958408\n",
      "1.0 \t 0.750445632798574\n",
      "10.0 \t 0.7605466428995841\n",
      "100.0 \t 0.750445632798574\n",
      "1000.0 \t 0.7450980392156863\n",
      "10000.0 \t 0.7415329768270945\n",
      "100000.0 \t 0.7391562685680333\n",
      "1000000.0 \t 0.7367795603089721\n",
      "10000000.0 \t 0.7361853832442068\n",
      "100000000.0 \t 0.7361853832442068\n",
      "1000000000.0 \t 0.7361853832442068\n",
      "10000000000.0 \t 0.7361853832442068\n"
     ]
    }
   ],
   "source": [
    "list_acc\n",
    "list_c=[]\n",
    "for key in list_acc.keys():\n",
    "    list_c.append(key)\n",
    "for key in list_acc:\n",
    "    print(key,'\\t',list_acc[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
